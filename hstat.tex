\documentclass[3p]{elsarticle}
\usepackage{amsmath,amssymb,bm}
\usepackage{graphicx,url,verbatim,slashbox,multirow,hyperref}
\usepackage{microtype}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{siunitx}

\newcommand{\EE}{\mathcal E}
\newcommand{\KK}{\mathsf K}
\newcommand{\PP}{\mathsf P}
\newcommand{\VV}{\bm V}
\newcommand{\R}{\mathbb R}
\newcommand{\ASM}{\mathrm{ASM}}
\newcommand{\RASM}{\mathrm{RASM}}
\newcommand{\bigO}{{\mathcal{O}}}
\newcommand{\abs}[1]{{\left\lvert #1 \right\rvert}}
\newcommand{\norm}[1]{{\left\lVert #1 \right\rVert}}
\newcommand{\tcolon}{{ : }}
\newcommand{\ip}[2]{{\left\langle #1, #2 \right\rangle}}

\DeclareMathOperator{\sspan}{span}
\DeclareSIUnit\year{a}
\sisetup{retain-unity-mantissa = false}

%\journal{Journal of Computational Physics}

\begin{document}
\begin{frontmatter}
  \title{Achieving textbook multigrid efficiency for hydrostatic ice sheet flow}

  \author[vaw]{Jed Brown} \ead{brown@vaw.baug.ethz.ch}
  \author[mcs]{Barry Smith} \ead{bsmith@mcs.anl.gov}
  \author[kaust]{Aron Ahmadia} \ead{aron.ahmadia@kaust.edu.sa}

  \address[vaw]{Versuchsanstalt f\"ur Wasserbau, Hydrologie und Glaziologie (VAW), ETH Z\"urich, 8092     Z\"urich, Switzerland}
  \address[mcs]{Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL     60439}
  \address[kaust]{KAUST-IBM Center for Deep Computing Research, Bldg \#1, Office 01-123,
    4700 King Abdullah University of Science and Technology,
    Thuwal Makkah 23955-6900,
    Kingdom of Saudi Arabia}

  \begin{abstract}
    The hydrostatic equations for ice sheet flow offer improved fidelity compared to the shallow ice approximation and shelfy stream approximation popular in today's ice sheet models, but it presents a serious bottleneck because it requires the solution of a 3D nonlinear system, as opposed to the 2D system present in SSA.  This system is posed on high-aspect domains with strong anisotropy and variation in coefficients, making it very expensive to solve using the methods currently employed.  This paper presents a multigrid Newton-Krylov solver for the hydrostatic equations that demonstrates textbook multigrid efficiency (an order of magnitude reduction in residual per iteration and solution of the fine-level system in a small multiple of the cost of a residual evaluation).  Scalability on Blue Gene/P is demonstrated and the method is compared to a variety of algebraic methods that are currently in use or have been proposed as viable alternatives.
  \end{abstract}
  \begin{keyword}
    hydrostatic, ice sheet, Newton-Krylov, multigrid, preconditioning
  \end{keyword}
  \thispagestyle{plain}
\end{frontmatter}

\section{Introduction}
The dynamic response of ice streams and outlet glaciers is poorly represented using the shallowness
assumptions inherent in the present generation of ice sheet models.  Accurate simulation of this
response is crucial for prediction of sea level rise and the inability of available models, based on
the shallow ice approximation (SIA)~\cite{hutter1983tgm} and shallow stream approximation
(SSA)~\cite{morland1987unconfined,weis1999theory}, to simulate these processes was cited as a major
deficiency in the IPCC AR4~\cite{ipcc2007ar4-syr}.

The hydrostatic equations were introduced by \cite{blatter1995vas} as a model of intermediate complexity between the full non-Newtonian Stokes system and the integrated SIA and SSA models.  A more precise analysis, including the limiting cases of fast and slow sliding, was given in \cite{schoof2010thin}.  Well-posedness was proven in \cite{colinge1999strongly} and approximation properties of finite element methods were analyzed in \cite{glowinski2003approximation,chow2004finite}.  The hydrostatic equations were used for transient simulation in \cite{pattyn2002tgr} and in 3D in \cite{pattyn2003ntd}, as well as subsequent work. Several models of this form were compared in \cite{pattyn2008beh}.

However, the use of hydrostatic equations in current models has been limited due to the cost of solving the 3D nonlinear system for velocity.  This cost comes from both slow convergence on the
nonlinearities (rheology and slip), and expensive linear solves using standard preconditioners such
as incomplete factorization and 1-level domain decomposition.  The poor linear solve performance is
attributable to the strong anisotropy and heterogeneity imposed by the rheology and geometry.

In the present work, we present a multigrid Newton-Krylov solver that demonstrates textbook
multigrid efficiency, characterized by convergence in a small multiple of the cost of a single
fine-level residual evaluation, and typically involving an order of magnitude reduction in residual
per multigrid (V or F) cycle.  The scheme converges quadratically on the nonlinearities, is rapidly
globalized using grid sequencing, is very robust to parameters and geometry, coarsens rapidly in
almost all cases, and exhibits excellent parallel scalability.  Our code is freely available as part
of the Portable Extensible Toolkit for Scientific computing (PETSc)~\cite{petsc-web-page}.

Section~\ref{sec:equations} presents the equations and discretization, Section~\ref{sec:solver} describes the solver, and Section~\ref{sec:examples} demonstrates performance and scalability with numerical examples.

\section{Equations and discretization}\label{sec:equations}
The hydrostatic equations are obtained from the non-Newtonian Stokes equations in the limit where
horizontal derivatives of vertical velocity are small.  Neglecting these terms allows
incompressibility to be enforced implicitly by eliminating pressure and vertical velocity, leaving a
system involving only horizontal components of velocity.  See \cite{schoof2010thin} for a rigorous
derivation and asymptotic analysis.

Consider an ice domain $\Omega \subset \R^3$ lying between a Lispschitz continuous bed $b \in C^0(\R^2)$ and surface $s
\in C^0(\R^2)$, with thickness $H = s-b$ bounded below by a positive constant\footnote{This singular
  limit is important in the case of grounded margins, but the present work does not pursue it.}  The
velocity $\bm u = (u,v) \in \bm V = \bm H^1(\Omega)$ satisfies conservation of momentum which,
omitting inertial and convective terms as is standard for ice sheets, is given by
\begin{align}\label{eq:momentum}
  - \nabla\cdot \left[ \eta
  \begin{pmatrix}
    4 u_x + 2 v_y & u_y + v_x & u_z \\
    u_y + v_x & 2 u_x + 4 v_y & v_z
  \end{pmatrix} \right] + \rho g \nabla s & = 0
\end{align}
where
\begin{align}\label{eq:viscosity}
  \eta(\gamma) = \frac B 2 (\epsilon^2 + \gamma)^{\frac{1-n}{2n}}
\end{align}
is the nonlinear effective viscosity with regularization $\epsilon$ and
\begin{align*}
  \gamma = u_x^2 + v_y^2 + u_xv_y + \frac 1 4 (u_y+v_x)^2 + \frac 1 4 u_z^2 + \frac 1 4 v_z^2
\end{align*}
is the second invariant.  Ice sheet models typically take $n=3$ as the power law exponent.  Equation
\eqref{eq:momentum} is subject to natural boundary conditions at the free surface and either no-slip
$\bm u = 0$ or power-law slip with friction parameter
\begin{align*}
  \beta^2(\gamma_b) = \beta_0^2 (\epsilon_b^2 + \gamma_b)^{\frac{m-1}{2}}
\end{align*}
where $\gamma_b = \frac 1 2 (u^2 + v^2)$, $\epsilon_b$ is regularization, and $m \in (0,1]$ is the
exponent that produces Navier slip for $m=1$, Weertman~\cite{weertman1957sliding} sliding for
$m=1/n$, and Coulomb slip as $m \to 0$.  In the present work, we define $\epsilon$ and $\epsilon_b$ using a strain rate of $\SI{1e-5}{\per\year}$ and a slip velocity of $\SI{1}{\metre\per\year}$ respectively.

To discretize this system with a finite element method, we introduce the test functions $\bm \phi
\in \VV$ and integrate by parts to produce the weak form: find $\bm u \in \VV$ such that
\begin{align}\label{eq:weak}
  \int_\Omega \nabla\bm\phi \tcolon \eta \bm 1 \tcolon
  \begin{pmatrix}
    4 u_x + 2 v_y & u_y + v_x & u_z \\
    u_y + v_x & 2 u_x + 4 v_y & v_z
  \end{pmatrix} + \bm\phi \cdot\rho g \nabla s
  + \int_{\Gamma_{\text{bed}}} \bm \phi \cdot \beta^2(\abs{\bm u}^2/2) \bm u = 0
\end{align}
for all $\bm \phi \in \VV$ where $\Gamma_{\text{bed}}$ is the slip portion of the bed.

Equation \eqref{eq:weak} was discretized on a topologically structured hexahedral grid using $Q_1$ finite elements and standard $2^3$-point Gauss quadrature.  Length, time, and mass units were chosen so that thickness, velocity, and driving stresses are $\bigO(1)$.  See Section~\ref{ssec:dirichlet} for details on the enforcement of Dirichlet boundary conditions.

The source code for our implementation is distributed as an example within PETSc~\cite{petsc-web-page} versions 3.1 and later\footnote{Upon unpacking the source, which can be downloaded from \href{http://mcs.anl.gov/petsc}{mcs.anl.gov/petsc}, see \texttt{src/snes/examples/tutorials/ex48.c}}.  Several generalizations of the tests from ISMIP-HOM~\cite{pattyn2008beh} are implemented, run with the option \texttt{-help} for a complete list of options.  Incidentally, our results for ``\SI{5}{\kilo\metre} test $C$'' in that paper (a slipperyness perturbation on a flat bed) agree to three significant figures with the ``Stokes'' results therein.  This is consistent with the asympotic analysis of \cite{schoof2010thin} which shows that slipperyness perturbations are not present in the leading order error terms for the hydrostatic equations (which is purely geometric), cf. \cite[Table~4 and Figure~8]{pattyn2008beh} where the ensemble range is nearly as large as the mean.

\section{Solver and implementation}\label{sec:solver}
We begin by writing the discretization of \eqref{eq:weak} as an algebraic system $F(U) = 0$ with
Jacobian $J(U)$.  This nonlinear system is solved with a Newton iteration which requires an
(approximate) solution $\delta U$ of
\begin{align}\label{eq:newton-step}
  J(U)\delta U = -F(U) .
\end{align}
Newton methods are quadratically convergent in the terminal phase, but may converge very slowly or
not at all in early phases.  Many applications of the present solver are in a time-stepping code
where the initial iterates start within the region of quadratic convergence, thus globalization
would rarely be a concern, but since a good initial iterate is not available in the present tests,
we use grid sequencing (solving the problem on a sequence of coarser grids) to produce an initial
iterate on the fine grid.

The Newton step \eqref{eq:newton-step} is solved by a Krylov method such as GMRES, for which the iteration count is highly dependent on quality of the preconditioner.  Since $J(U)$ is symmetric positive definite (SPD), methods such as conjugate gradients could be used, however this work always uses GMRES because it allows the use of nonsymmetric preconditioners, and the iteration counts are always kept very low so that storage and restarts are not an issue.  As an SPD system, a wide variety of preconditioners are available, however, viscosity contrasts and strong anisotropy cause most preconditioners to perform poorly.  The rest of this section describes the methods used to produce a scalable algorithm in spite of these difficulties.

\subsection{Anisotropy}
The aspect ratio of outlet glaciers (the regions of ice sheets with greatest physical interest) range from $\bigO(1)$ to over $100$.  The nonlinear constitutive relation \eqref{eq:viscosity} produces three to four orders of magnitude variation in viscosity (usually with fastest variation in the vertical) and the Newton linearization of \eqref{eq:weak} produces additional anisotropy, effectively collapsing the conductivity tensor in the direction of the velocity gradient.

For systems with a priori known anisotropy, semi-coarsening has been successful for attaining
satisfactory multigrid performance even with weak smoothers like SOR, but semi-coarsening is
unattractive for two reasons.  Firstly, semi-coarsening in the vertical direction would necessitate
many levels because it only reduces the problem size by a factor of 2 on each coarsening (instead of
8 for isotropic coarsening), and the fully-coarsened problem would still be far too large for a
direct solver, necessitating further coarsening in the horizontal direction.  The presence of many
levels leads to more synchronization in parallel which is detrimental to scalability and makes
performance more sensitive to network latency.  Second, viscosity contrasts and anisotropy
unaligned with the grid arise when the friction parameter $\beta^2$ is not smooth, which is the standard
case when studying the migration of ice stream margins as the bed transitions from frozen (no-slip
or very high friction) to temperate and very slippery depending on subglacial hydrology.

In order to coarsen isotropically even on high-aspect domains, we order the unknowns so that columns
are contiguous with a natural block size of 2
(i.e. $\{u_{i,j,k},v_{i,j,k},u_{i,j,k+1},v_{i,j,k+1},\dotsc\}$ where $k$ is the index which is
increasing in the vertical direction) and not decomposed in parallel.  This decomposition is
reasonable since the number of vertical levels used in simulations is typically between 10 and 100,
and convenient since it is compatible with decompositions used by other climate model components.

With this ordering, zero-fill incomplete factorization effectively performs an exact solve of the
column since all the neglected fill (relative to an exact factorization) is caused by the coupling
with adjacent columns.  Pure line smoothers were also tried as a smoother on the finest level, but
robustness was significantly impacted and the memory benefits were deemed insufficient to pursue
further.

In scenarios with little sliding (frozen or sticky bed) and elements that are wide compared to the
ice thickness, a lubrication approximation known as the ``shallow ice approximation'' is valid,
allowing the velocity field to be determined locally from surface slope and a column integral.
Since incomplete factorization with column ordering provides nearly exact coupling in the vertical,
it is an effective preconditioner in such scenarios with no coarse level.  Indeed, with a typical
ice thickness of \SI{1}{\kilo\metre} resting on a frozen bed and elements \SI{5}{\kilo\metre} on
a side, block Jacobi with zero-fill incomplete Cholesky converges to relative tolerance of $10^{-5}$
in about 10 Krylov iterations independent of the horizontal extent of the domain (number of elements
in the horizontal), independent of the number of elements in the vertical, and independent of the
number of subdomains (provided they do not get too small, there is some degredation when subdomain
size approaches a single column).  However, none of these favorable performance characteristics
remain when the elements become small relative to the ice thickness or when the bed becomes
slippery, since the usual $\bigO\big((L/H)^2\big)$ condition number for second order elliptic problems
preconditioned by 1-level Schwarz methods with subdomains of size $H$ kicks in.  Indeed, we have found low-fill incomplete factorization to be nearly unusable as part of a 1-level additive Schwarz method, even on quite small problems, as investigated in Section~\ref{sec:1level}.

%%%%%%%%%%%%%%%%%%  It appears that this section is wrong and shifting is not the issue (just a PETSc bug in an older version of PETSc)
%We have observed a weakness of incomplete Cholesky factorization that makes it nearly unusable as a stand-alone preconditioner for this problem.  The Jacobian, including each subdomain problem in an block Jacobi or additive Schwarz method, is symmetric positive definite, which guarantees that a full Cholesky factorization can proceed with any ordering and will always find positive pivots~\cite{trefethen1997numerical}.  This property does not hold for incomplete factorization as discussed in~\cite{kershaw1978incomplete}.  Indeed, we find that strongly anisotropic problems (whether the anisotropy is due to mesh aspect ratio or strong rheologic nonlinearity) produce Jacobians for which incomplete Cholesky with any standard static pivoting (e.g. natural ordering, reverse Cuthill-McKee, nested dissection) and any reasonably amount of fill (e.g. 0--8 levels) produces negative pivots.  As is standard, these negative values can be ``shifted'' to a positive value so that the algorithm can continue.  Empirically, this shifting causes both a local error that may be cleaned up rapidly by the Krylov method and a nonlocal error that is disruptive to 1-level domain decomposition methods, and increasing overlap or fill has limited benefit or is actually harmful.  This effect is explored in Section~\ref{sec:1level}.

\subsection{Dirichlet boundary conditions}\label{ssec:dirichlet}
Multigrid is often sensitive to the enforcement of boundary conditions.  Ideally, Dirichlet
conditions would be completely removed from the solution space, but doing so complicates grid
management on structured grids, so instead we leave these degrees of freedom in the system but
decouple them from the other equations.  During residual evaluation in the finite element context,
this corresponds to evaluating integrals with the Dirichlet condition satisfied exactly, and setting
the residual on the Dirichlet nodes to be equal to a multiple of the current solution.  With this
scheme, all rows and columns of the Jacobian corresponding to Dirichlet nodes are zero except for a
single diagonal entry, thus the system retains symmetry and satisfaction of the Dirichlet conditions
does not interfere with solving the other equations.  For good multigrid performance, the diagonal
entry should be similar to the diagonal entry of the Jacobian for nearby nodes.  To ensure this, we
set the residual at Dirichlet nodes to
\begin{align}\label{eq:dirichlet-scale}
  f_u &= 2 \eta (4 h_yh_z/h_x + h_x h_z/h_y + h_x h_y/h_z) u \\
  f_v &= 2 \eta (h_yh_z/h_x + 4 h_x h_z/h_y + h_x h_y/h_z) u
\end{align}
where $h_x,h_y,h_z$ are the local element dimensions.  This scaling produces the same diagonal
entries that would appear if the domain was extended so that constant viscosity momentum equations
appeared at the formerly Dirichlet nodes.

\subsection{Matrices}
The most expensive operations are Jacobian assembly and sparse matrix kernels.  The former involves evaluation of transcendental functions and quadrature loops.  While transcendental functions take most of the time for residual evaluation, they are less significant than quadrature loops for assembly.  The quadrature loops were explicitly vectorized using SSE2 intrinsics which led to a 30\% speedup on Core 2 and Opteron architectures using both GNU and Intel compilers.  There was no manual vectorization for the Blue Gene/P results quoted in Section~\ref{sec:pscaling}.

Assembly costs could be further mitigated by recomputing it less frequently, either in a modified Newton method (degrades nonlinear convergence rate) or by applying the current operator matrix-free by finite differencing the residual or using automatic differentiation in which case only the preconditioner is lagged.  These are runtime options in the present code, but not a clear benefit so have not been pursued in the present work.  If matrix-free application of the true Jacobian is used, several other preconditioning options become available without impacting the nonlinear convergence rate.  One could assemble only the block-tridiagonal column coupling, ignoring horizontal coupling, thus saving the memory for the finest level(s). Additionally, a truly 2D coarse problem can be defined by using the shallow stream equations~\cite{morland1987unconfined,weis1999theory,schoof2006variational} and restriction operators defined by integrating the entire column.  These possibilities are also runtime options, but have not exhibited a level of robustness comparable to the more conventional methods pursued here.

The Jacobian is always symmetric positive definite and has a natural block size of 2, so we use a symmetric block format (PETSc's SBAIJ).  This format stores one column index per $2\times 2$ block in the upper triangular part of the matrix and therefore uses about half the storage of the nonsymmetric BAIJ format which in turn uses 25\% less memory than a scalar format (AIJ). Multiplication for symmetric storage requires twice as much parallel communication as nonsymmetric storage, albeit with the same number of messages. In return, the diagonal part of a parallel decomposition does twice as much work per matrix entry and thus achieves higher throughput as shown in Table~\ref{tab:sbaij}.

\begin{table}
  \centering\caption{Throughput (Mflop/s) for different matrix formats on Core 2 Duo (P8700) and Opteron 2356 (two sockets). \texttt{MatSolve} is a forward- and back-solve with incomplete Cholesky factors.  The AIJ format is using ``inodes'' which unrolls across consecutive rows with identical nonzero pattern (pairs in this case).}\label{tab:sbaij}
  \begin{tabular}{l|c|c|c|c|c|c}
    \multirow{2}{*}{\backslashbox{kernel}{format}} & \multicolumn{3}{c|}{Core 2, 1 thread} & \multicolumn{3}{c}{Opteron, 4 threads} \\
                      & AIJ & BAIJ & SBAIJ & AIJ  & BAIJ & SBAIJ \\ \hline
    \texttt{MatMult}  & 812 & 985  & 1507  & 2226 & 2918 & 3119  \\
    \texttt{MatSolve} & 718 & 957  & 955   & 1573 & 2869 & 2858  \\
  \end{tabular}
\end{table}

\section{Numerical examples}\label{sec:examples}
\subsection{Algorithmic Scalability}\label{sec:ascaling}
We consider three model problems inspired by the periodic domain ISMIP-HOM~\cite{pattyn2008beh} tests.  All use surface $s(x,y) = -x\sin \alpha$ where $\alpha$ is the surface slope (the coordinate system is not rotated) and a bed similar to $b_A(x,y) = s(x,y) - \SI{1000}{\metre} + \SI{500}{\metre} \cdot \sin\hat{x}\sin\hat{y}$ for $(x,y) \in [0,L)^2$ with $\hat{x} = 2\pi x/L,\hat{y}=2\pi y/L$.  Test $X$ uses bed $b_X = b_A$ and stickyness parameter
\begin{align*}
  \beta_X^2(x,y) =
  \begin{cases}
    \SI{2000}{\pascal\year\per\metre}, & \text{if } r = \abs{(\hat x,\hat y) - (\pi,\pi)} < 1 \\
    0, & \text{otherwise}
  \end{cases}
\end{align*}
which is free slip except for a sticky circle at the center of the domain which is not aligned with the grid.  This problem exhibits shear localization at the edges of the sticky region and is most extreme at high aspect ratio.  We choose $L = \SI{80}{\kilo\metre}$ and $\alpha = 0.05^\circ$\footnote{This problem may be run with the options \texttt{-thi\_hom X -thi\_L 80e3 -thi\_alpha 0.05}, the other cases can be selected with similar options.} which produces velocities from \SIrange{0.9}{47}{\kilo\metre\per\year}.  A visual representation of the nonlinear solve process is shown in Figure~\ref{fig:xgridseq}.  This was run on 8 processors starting from a coarse grid of $16\times 16\times 1$, refining twice in the horizontal by factors of 2 in both $x$ and $y$, then three times in the vertical by factors of 8 each to reach a fine mesh of $64\times 64\times 513$ which has elements of nominal dimension $1250\times 1250\times 1.95$ meters.  In this example and the next one, Luis Chac\'on's variant of the Eisenstat-Walker~\cite{eisenstat1996cft} method was used to automatically adjust linear solve tolerances as the nonlinear solve converges.  Solving the linear systems to higher tolerance would have little impact on the number of nonlinear iterations and would be visible in the form of more $\times$ marks below the solid lines.  That most $\times$ marks lie on the solid line for nonlinear residual is an indication that effort is well balanced between linear and nonlinear solves.

Note that approximately 10 linear V-cycles on the fine level are required to reduce the residual by 10 orders of magnitude.  We remark that Picard iteration takes at least 50 iterations to reach this tolerance (sometimes many more, cf. \cite{desmedt2010using} in which hundreds or thousands of iterations were needed for an easier problem).  Additionally, each linear solve for this fine-level problem requires hundreds or thousands of iterations with a 1-level additive Schwarz method, see Section~\ref{sec:1level} which considers a smaller problem.

\begin{figure}
  \centering\caption{Grid-sequenced Newton-Krylov solution of test $X$.  The solid lines denote nonlinear iterations and the dotted lines with $\times$ denote linear residuals.}\label{fig:xgridseq}
  \includegraphics{figures/x-80km-m16p2l6-ew}
  % petscplot -m paper -t snes --solve-spec '1:' x.80km.m16p2l6.ew.log -o ../figures/x-80km-m16p2l6-ew.pdf
\end{figure}

Test $Y$ places a \SI{200}{\metre} tower with vertical walls on the top of each hump and uses an uncorrelated, but smoothely varying stickyness resembling a dimpled sombrero,
\begin{align*}
  b_Y(x,y) &=
  \begin{cases}
    b_A(x,y), & \text{if } b_A(x,y) < \SI{-700}{\metre} \\
    b_A(x,y) + \SI{200}{\metre}, & \text{otherwise}
  \end{cases} \\
  \beta_Y^2(x,y) &= \SI{1000}{\pascal\year\per\metre} \cdot (1 + \sin(\sqrt{16r})/\sqrt{10^{-2} + 16r}\cos\frac{3\hat x}{2}\cos\frac{3\hat y}{2}.
\end{align*}
This tests the quality of the coarse grids even when large geometric errors are committed.  Note that the hydrostatic equations cannot be considered valid in this regime since the topography is too abrupt.  However, such topography is present in reality so we may still desire an efficient solver.  Figure~\ref{fig:testy} depicts the solve for this problem in a \SI{10}{\kilo\metre} square domain.  Due to the successively better resolution of the ``cliff'', performance deteriorates on each level as can be seen by the closer spacing of linear solve marks $(\times)$.  However, it is entirely acceptable up to level 3 where the elements are approximately \SI{12}{\metre} thick and stretched to reach over a \SI{200}{\metre} cliff in \SI{125}{\metre} horizontal.  On the finest level, they are \SI{6}{\metre} thick and stretch over the cliff in \SI{62}{\metre} horizontal, a slope of \SI{73}{\degree}.  The approximation properties of such elements is quite poor and, considering that the continuum equations are completely invalid here, we believe this resolved topography is significantly rougher than will needed in applications.

\begin{figure}
  \centering\caption{Grid sequenced Newton-Krylov convergence for test $Y$.}\label{fig:testy}
  \includegraphics{figures/y-10km-m10p6l5-ew}
  % petscplot -m paper -t snes --solve-spec '1:' y.10km.m10p6l5.ew.log -o ../figures/y-10km-m10p6l5-ew.pdf
\end{figure}

Finally, test $Z$ sets $b_Z = b_A$, $\beta_Z^2 = \beta_X^2$, and nonlinear sliding with exponent $m=0.3$, and is a regime where the hydrostatic equations are valid, provided the wavelength $L$ is not too small.  We use this case to explore linear solve performance in Figure~\ref{fig:linear}.

\begin{figure}
  \centering\caption{Average number of Krylov iterations per nonlinear iteration.  Each nonlinear system was solved to a relative tolerance of $10^{-2}$.}\label{fig:linear}
  \includegraphics{figures/linear4-mine}
  % ~/petscplot/petscplot -m paper --width-pt 380 -t algorithmic z.m6p4l5.newton.icc.log : z.m6p4l5.picard.asm8.icc1.log : z.m6p4l6.picard.icc.log : z.m6p4l6.mult.n8.o0.r2.log --legend-labels 'Newton, ICC(0), serial:Picard ASM(1)/ICC(1), 8 subdomains:Picard ICC(0) serial:Newton, V-cycle, 8 subdomains' -o ../figures/linear4-mine.pdf
\end{figure}

\subsection{Parallel Scalability on Blue Gene/P}\label{sec:pscaling}
We investigate strong scaling using test $Z$ at \SI{80}{\kilo\metre} with basal friction exponent $m= 0.3$ on Shaheen, a Blue Gene/P at the KAUST Supercomputing Laboratory.  Two problem sizes are solved, with coarse meshes of $16\times 16\times 3$ and $32\times 32\times 3$ respectively.  Both use 5 levels of isotropic refinement to reach target meshes of $256\times 256\times 48$ and $512\times 512\times 48$, the latter with nominal element sizes of $156\times 156\times 21$ meters.  The coarse problems are solved redundantly in each case, and also using the $XX^T$ direct solver of~\cite{tufo2001fast} (TFS) which exhibits significantly better scalability.  Figure~\ref{fig:shaheen} shows strong (fixed global problem size) and weak (fixed subdomain problem size) scalability. For the latter, the size of the coarse grid problem was held constant and additional levels were added.

\begin{figure}
  \centering\caption{Strong (top) and weak scaling on Shaheen, the straight lines on the strong scaling plot have slope $-1$ which is optimal.  Both use grid sequencing, but the strong scaling case shows only the nonlinear solve on the finest level while the weak scaling plot includes all the grid-sequenced problems.}\label{fig:shaheen}
  \includegraphics{figures/shaheen-strong} \\
  % /petscplot -m paper --events SNESSolve --stages 3 -t strong fast_strong_*_16_4_*.out : fast_strong_*_32_4_*.out : strong_tfs_*.out --legend-labels '$256\times 256\times 48$ Redundant:$512\times 512\times 48$ Redundant:$512\times 512\times 48$ TFS' -o ../figures/shaheen-strong.pdf
  \includegraphics{figures/shaheen-weak}
  % ~/petscplot/petscplot -m paper --events SNESSolve --stages 1 -t weak weak_[345]*.out --title '' --legend-labels '$32\times 32\times 3$ coarse level, Redundant' -o ../figures/shaheen-weak.pdf --width-pt 234
\end{figure}

\subsection{Algebraic methods}\label{sec:1level}
Building a geometric hierarchy with rediscretization on coarse levels adds software complexity that many developers of numerical models do not want to deal with.  In this section, we summarize the performance characteristics of several popular algebraic methods.  We consider test $X$, $L = \SI{80}{\kilo\metre}, \alpha=0.03$, with $40\times 40\times 12$ elements distributed over four processes, and compare several 1-level domain decomposition methods, two algebraic multigrids, and field-split approaches to our multigrid method.  This problem is challenging for the standard Newton iteration which requires 37 iterations and should be accompanied by grid sequencing for efficiency. However, it takes the problem through a range of nonlinearities and thus the number of Krylov iterations to solve with a relative tolerance of $10^{-5}$, presented below, is a good test of the linear solver.

We first consider 1-level domain decomposition methods with incomplete factorization, which are currently used to solve the hydrostatic equations by \cite{seacism,issm} among others.  To keep iteration counts representative, we use full GMRES (no restart) with modified Gram-Schmidt orthogonalization (note that neither is practical for production use).  Conventional symmetric additive Schwarz is denoted $\ASM(k)$ where $k$ is the overlap and restricted additive Schwarz~\cite{cai1999restricted} is denoted $\RASM(k)$.  The average number of GMRES iterations per Newton is shown in Table~\ref{tab:1level}.  Note that increasing overlap has no benefit when incomplete subdomain solvers are used.

\begin{table}
  \centering\caption{Average number of GMRES iterations per Newton for 1-level domain decomposition with different overlap and fill.  Negative pivots appeared frequently in all cases where incomplete factorization was used.}\label{tab:1level}
  \begin{tabular}{l|c|c|c|c}
    \backslashbox{decomp.}{subdomain} & ICC(0) & ICC(1) & ICC(4) & Cholesky \\ \hline
    Block Jacobi & 367 & 315 & 220 & 97 \\
    $ASM(1)$     & 508 & 441 & 296 & 59 \\
    $RASM(1)$    & 368 & 306 & 190 & 52 \\
    $ASM(2)$     & 521 & 445 & 316 & 44 \\
    $RASM(2)$    & 365 & 305 & 189 & 38 \\
  \end{tabular}
\end{table}

The parallel algebraic multigrid packages ML~\cite{ml-guide} and BoomerAMG~\cite{henson2002bpa} provide potentially scalable alternatives.  ML is based on smoothed aggregation, tends to coarsen very rapidly, and provides its restriction and coarse-level matrices to PETSc so that that elaborate smoothers can be used.  ML does not converge for this problem with standard options, but with FGMRES on the outside of the V-cycle and GMRES(1) with $\RASM(1)$ as the smoother, using ICC(0) for the subdomain solve except on level 1 where a direct solve was used, we see 34 V-cycles per Newton.  ML needs only three levels to reach a coarse level with 144 degrees of freedom.  BoomerAMG is a classical multigrid which tends to coarsen slowly on anisotropic problems and does not expose the internal details, so smoother choices are limited.  BoomerAMG needs 7 levels to reach a coarse grid with 663 degrees of freedom, and averages 76 iterations per Newton.  There were other somewhat challenging problems for which BoomerAMG was competitive in terms of iteration count, but the setup costs and required number of levels was always large.

Another approach to solving multi-component problems is to split the components and solve scalar problems for each in hopes that the scalar problems can be more readily handled by available software such as algebraic multigrid.  The split problems can be combined additively, multiplicatively, or symmetric multiplicative.  Unlike most Schwarz methods, additive is not typically implemented to expose concurrency, but it is simpler to implement in a matrix-light way because only the ``self-coupling'' terms need to be made available.  Multiplicative methods need to apply the off-diagonal submatrix and the most efficient way to apply it is usually by assembling it, but the submatrix can also be applied by finite differencing the full residual.  The results, shown in Table~\ref{tab:fieldsplit}, are uninspiring especially when considering that field-split creates additional synchronization points and attains lower throughput since it works with scalar matrices instead of block matrices (see Table~\ref{tab:sbaij}).

\begin{table}
  \centering\caption{Field-split preconditioning with different ways of combining the splits and different solvers within the splits.  BoomerAMG used 7 levels and ML had 3 with the same solver parameters as discussed in the text for the coupled approach.}\label{tab:fieldsplit}
  \begin{tabular}{l|ccc}
    Solver in splits    & Additive & Multiplicative & Sym. multiplicative \\ \hline
    Cholesky            & 19       & 9.9            & 9.3                 \\
    ML                  & 41       & 34             & 30                  \\
    BoomerAMG           & 89       & 83             & 78                  \\
    $\RASM(1)$+Cholesky & 186      & 173            & 84
  \end{tabular}
  %  for ft in additive multiplicative symmetric_multiplicative; do ~/usr/bin/mpiexec -n 4 ./e -M 40 -P 13 -thi_L 80e3 -thi_hom x -thi_alpha 0.03 -thi_nlevels 1 -{snes,ksp}_converged_reason -pc_type fieldsplit -pc_fieldsplit_type $ft -fieldsplit_0_pc_type ml -fieldsplit_0_pc_ml_maxnlevels 3 -fieldsplit_0_mg_levels_ksp_type gmres -fieldsplit_0_mg_levels_pc_type asm -fieldsplit_0_mg_levels_pc_asm_type restrict -fieldsplit_0_mg_levels_1_sub_pc_type cholesky -fieldsplit_1_pc_type ml -fieldsplit_1_pc_ml_maxnlevels 3 -fieldsplit_1_mg_levels_ksp_type gmres -fieldsplit_1_mg_levels_pc_type asm -fieldsplit_1_mg_levels_pc_asm_type restrict -fieldsplit_1_mg_levels_1_sub_pc_type cholesky; done
\end{table}

A good geometric multigrid for this problem uses 4-levels with a coarse grid of $10\times 10\times 2$ elements which is semi-refined twice by a factor of 2 in the horizontal, then by a factor of 6 in the vertical to reach the target $40\times 40\times 12$ grid.  Block Jacobi is used as the smoother on levels, paired with either an exact or inexact subdomain solve.  A direct solver is always used on the coarsest level and for the subdomain solvers on the next coarsest level (level 1, these problems are very small so the direct solve is cheap).  When a direct solve is used for subdomain problems on levels 2 and 3, an average of 8.9 Krylov iterations are needed per Newton.  Replacing these direct solves with zero-fill incomplete Cholesky raises the iteration count to 9.6.  Subsequently increasing overlap from 0 to 1 on level 1 produces 6.9 for $\RASM(1)$ and 11.9 for $\ASM(1)$.  Using overlap on all levels produces 5.9 iterations for $\RASM(1)$ and 20.2 for $\ASM(1)$.  We cannot explain why symmetric additive Schwarz performs so poorly in this problem, but the other numbers are robust to changes in resolution, spatial domain, and number of processes.
When using Galerkin coarse operators with this last $\RASM(1)$ method, the average iteration count goes up to 54 (from 5.9), suggesting that Galerkin coarse operators may be responsible for some of the poor robustness exhibited by the algebraic multigrids.

A different refinement involves a $10\times 10\times 1$ coarse grid and semi-refines twice by a factor of 2 in the horizontal, then by a factor of 12 in the vertical to reach the same target grid.  The coarse levels are smaller in this case and the average iteration counts are 10.3, 10.1, and 6.9 for block Jacobi, $\ASM(1)$, and $\RASM(1)$ on level 1 (always block Jacobi on finer levels), with ICC(0) used for all subdomain solves finer than level 1.  When using grid sequencing and this last method, the problem can be solved in 7 Newton iterations on the fine level and an average of 5.4 V-cycles per Newton. If the Eisenstat-Walker method is used to avoid oversolving, it takes 8 Newton iterations with a total of 12 V-cycles.

\section{Discussion}
We have presented a grid-sequenced Newton-Krylov multigrid algorithm for solving the hydrostatic equations for ice sheet flow.  It demonstrates ``textbook multigrid efficiency'' under extreme topography and basal conditions, and offers $\bigO(1000)$ speedups relative to Picard linearization and 1-level domain decomposition with incomplete factorization which is currently used to solve these equations~\cite{seacism,issm,johnson2007modeling,desmedt2010using,pattyn2003ntd}.

\nonumber\section{Acknowledgments} This work was supported by Swiss National Science Foundation Grant 200021-113503/1
and U.S. Department of Energy Office of Advanced Scientific Computing Research grant {\bf ?? SISPHUS ??} and Shaheen Supercomputing Laboratory at KAUST.

\bibliographystyle{model1-num-names}
\bibliography{flow}
\end{document}
